---
title: "wine"
author: "Curtis Easton"
date: "2024-11-20"
output: html_document
---

```{r setup}

options(repos = c(CRAN = "https://cloud.r-project.org"))

```

## load data

```{r}

# Load Necessary Libraries
library(dplyr) # For data manipulation

# Load the Red Wine Data
red_wine = read.csv("winequality-red.csv", sep = ";")

# Load the White Wine Data
white_wine = read.csv("winequality-white.csv", sep = ";")

# Add a Categorical Variable for Wine Type
red_wine$type = "Red"
white_wine$type = "White"

# Combine the Two Datasets
wine_data = bind_rows(red_wine, white_wine)

# Convert 'type' to a Factor
wine_data$type = as.factor(wine_data$type)

nrow(wine_data)

# View(wine_data)


```

## Additional Preliminary Overview of the Dataset

```{r}
table(wine_data$quality) # Distribution of `quality`
```


## Histogram of quality

```{r}
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

ggplot(wine_data, aes(x = quality, fill = type)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Quality by Type", x = "Quality", y = "Count") +
  theme_minimal()
```

Comments:

`quality` values are concentrated around 5, 6, and 7.

White wines dominate `quality` score 6.

Red wines are nearly evenly split between `quality` scores 5 and 6.

## Visualize correlations through a heatmap
```{r}
if (!require(corrplot)) install.packages("corrplot")
library(corrplot)
numeric_data = wine_data %>%
  select(-type, -quality) # Considers numeric predictors only
cor_matrix = cor(numeric_data, use = "complete.obs")


corrplot(cor_matrix, method = "color", addCoef.col = "black", 
         tl.cex = 0.8, number.cex = 0.7, main = "Correlation Heatmap")
```

Comments:

TODO: Insert comments about heatmap.

## Histograms for numeric predictors
```{r}
numeric_columns = names(numeric_data)
for (col in numeric_columns) {
  print(
    ggplot(wine_data, aes_string(x = col)) +
      geom_histogram(fill = "skyblue", color = "black", bins = 30) +
      labs(title = paste("Histogram of", col), x = col, y = "Frequency") +
      theme_minimal()
  )
}
```

Comments:

Some variables, like `fixed.acidity`, `citric.acid`, `alcohol`, and `pH`, show near-normal distributions, which is desirable.

Many numeric variables are skewed (including `volatile.acidity`, `residual.sugar`, `chlorides`, `sulphates`, `chlorides`, and `residual.sugar`). We'll consider transformations like log or square root.

## Exploring the Relationship Between Type and Quality

# Boxplot of quality by type
```{r}
ggplot(wine_data, aes(x = type, y = quality, fill = type)) +
  geom_boxplot() +
  labs(title = "Boxplot of Quality by Type", x = "Type", y = "Quality") +
  theme_minimal()
```

Comments:

The median values of `quality` are slightly different between red and white wines, with red wines having a smaller range of quality scores compared to white wines.


## Transforming Skewed Variables

```{r}
wine_data = wine_data %>%
  mutate(
    log_residual_sugar = log1p(residual.sugar),  # Use log1p to handle 0 values
    log_chlorides = log1p(chlorides),
    log_sulphates = log1p(sulphates)
  )
```

## Visualizing Transformed Variables
```{r}
ggplot(wine_data, aes(x = log_residual_sugar)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Histogram of Log-Transformed Residual Sugar", 
       x = "Log(Residual Sugar + 1)", 
       y = "Frequency") +
  theme_minimal()
```

```{r}
ggplot(wine_data, aes(x = log_chlorides)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Histogram of Log-Transformed Chlorides", 
       x = "Log(Chlorides + 1)", 
       y = "Frequency") +
  theme_minimal()
```

```{r}
ggplot(wine_data, aes(x = log_sulphates)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Histogram of Log-Transformed Sulphates", 
       x = "Log(Sulphates + 1)", 
       y = "Frequency") +
  theme_minimal()
```

Comments:

We performed log transformations on the skewed variables, but the histograms of the transformations don't suggest the tranformation really improved the situation. Below, we'll compare two full models - one with original predictors and the other with log transformed predictors. Based on the histograms, our hunch is that the transformed predictors won't improve the model.

## Placeholder: Data Partitioning (for Predictive Models)
## Train-Test Data Split

```{r}
# Modified from Homework Week 11, Exercise 3
set.seed(420)
wine_trn_idx  = sample(nrow(wine_data), size = trunc(0.80 * nrow(wine_data)))
wine_trn_data = wine_data[wine_trn_idx, ]
wine_tst_data = wine_data[-wine_trn_idx, ]
wine_trn_data$type = as.factor(wine_trn_data$type)
wine_tst_data$type = as.factor(wine_tst_data$type)
```

## Fit a Full Additive Model
```{r}
lm_full_original = lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + 
    residual.sugar + chlorides + free.sulfur.dioxide + 
    total.sulfur.dioxide + density + pH + sulphates + 
    alcohol + type, data = wine_data)

summary(lm_full_original)
```

## Fit the Full Model with Transformed Variables
```{r}
lm_full_transformed = lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + 
    log_residual_sugar + log_chlorides + free.sulfur.dioxide + 
    total.sulfur.dioxide + density + pH + log_sulphates + 
    alcohol + type, data = wine_data)

summary(lm_full_transformed)
```

## Comparing Full Models (Original Variables vs. Transformed Variables)
```{r}
AIC(lm_full_original, lm_full_transformed)
BIC(lm_full_original, lm_full_transformed)
```

Comments:

As anticipated, the full model with transformed predictors is not better than the full model with original predictors.

For the two models, the differences between the Residual Standard Errors, the Adjusted R-squared, and F-Statistic are all negligible.

Both AIC and BIC slightly favor the original model over the transformed model.

## Residual Diagnostics for lm_full_original

```{r}
plot(lm_full_original, which = 1, main = "Residuals vs Fitted")
```
```{r}
plot(lm_full_original, which = 2, main = "Normal Q-Q Plot")
```

Comments:

We'll attempt to address some of the potential linear model assumption violations by experimenting with interactive and polynomial terms.

We'll try `alcohol` and `density` as second-order polynomials. Both appear to be significant predictors, and using polynomials will amplify their influence on `quality`.

It also makes sense to explore interactive relationships involving `type`. Type is the sole categorical predictor. We'll see how it interacts with the two influential numerical predictors, `alcohol` and `density`.

## Add Interaction Terms and Polynomial Terms
```{r}
# Interactions: Include interactions involving 'type' and 'alcohol' or 'density'
# Polynomial: Include quadratic terms for 'alcohol' and 'density' to capture non-linear effects
lm_with_interactions = lm(
  quality ~ fixed.acidity + volatile.acidity + citric.acid +
    residual.sugar + chlorides + free.sulfur.dioxide +
    total.sulfur.dioxide + density + I(density^2) +
    pH + sulphates + alcohol + I(alcohol^2) +
    type + type:alcohol + type:density,
  data = wine_data
)

summary(lm_with_interactions)
```

## Address Heteroscedasticity with Log Transformation of the Response
```{r}
wine_data$log_quality = log(wine_data$quality)
lm_with_log_response = lm(
  log_quality ~ fixed.acidity + volatile.acidity + citric.acid +
    residual.sugar + chlorides + free.sulfur.dioxide +
    total.sulfur.dioxide + density + I(density^2) +
    pH + sulphates + alcohol + I(alcohol^2) +
    type + type:alcohol + type:density,
  data = wine_data
)

summary(lm_with_log_response)
```

Comments:

Compared to the full additive model, this revised model improves slightly upon Residual Standard Error and Adjusted R-squared.

Looking specifically at the polynomial terms, the small Pr(>|t|) values for `I(density^2)` and `I(alcohol^2)` suggest that they benefit the model.

The Pr(>|t|) for `density:typeWhite` suggests that `density` impacts `quality` differently, depending on the `type` of wine. The Pr(>|t|) for `alcohol:typeWhite` is much higher, however, suggesting we might want to remove this interactive term from our model.

As was the case in our previous models, `citric.acid` doesn't look like a significant predictor, and we should probably remove it.

## Diagnostic Plots for the Model with Log-Transformed Response
```{r}
plot(lm_with_log_response, which = 1, main = "Residuals vs Fitted")
```

```{r}
plot(lm_with_log_response, which = 2, main = "Normal Q-Q Plot")
```

Comments:

The Normal Q-Q Plot looks especially bad on the left end. Using a log transformation on the response variable doesn't look like a promising path. We'll continue to iterate on the predictors and hope to find a model that better abides by linear model assumptions.

## Refine the model by removing `citric.acid` and `alcohol:typeWhite`
```{r}
lm_refined = lm(
  quality ~ fixed.acidity + volatile.acidity +
    residual.sugar + chlorides + free.sulfur.dioxide +
    total.sulfur.dioxide + density + I(density^2) +
    pH + sulphates + alcohol + I(alcohol^2) +
    type + type:density,
  data = wine_data
)

summary(lm_refined)
```

Comments:

Removing `citric.acid` and `alcohol:typeWhite` did not harm the model's performance.

Other variables remain significant.

## Check multicollinearity with VIF
```{r}
if (!require(car)) install.packages("car")
library(car)
vif_values = vif(lm_refined)
vif_values

high_vif = vif_values[vif_values > 5]
high_vif
```

Comments:

As expected variables with polynomial terms (`density` and `alcohol`) are inherently collinear with their base terms.

The interactive term, `density:typeWhite` is collinear with its component terms.