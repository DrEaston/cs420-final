---
title: 'Data Analysis Project'
author: "STAT 420, Fall 2024, C. Easton, A. Roh, T. Toter, A. Treptow"
date: 'December xx, 2024'
output:
  html_document:
    theme: readable
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: 2
urlcolor: cyan
---

# Introduction

In this data analysis study we applied the principles we learned during the semester to iterate on a linear regression model.

For our case study, we chose a "Wine Quality" dataset from the UC Irvine Machine Learning Repository. The data relates to red and white variants of the Portuguese vinho verde wine samples. We drew the data from the following site:

  https://archive.ics.uci.edu/dataset/186/wine+quality

Each row in the dataset of wine samples contains a record of 11 numerically measured physicochemical attributes, such as acidity, residual sugar, chlorides, and pH. We combined two datasets (one for white wine and one for red wine), resulting in an additional categorical attribute for wine type.

The 12 attributes listed above served as our source predictor variables. A final attribute from the dataset measures quality, and serves as our response variable. Each quality measurement is a subjectively-assigned integer, ranging from 1 to 10. Our goal was to build a model that could use the objectively measured predictors as inputs to estimate how a human would rate each wine.

# Methods

## Setup

```{r setup}
options(repos = c(CRAN = "https://cloud.r-project.org"))
```

## Load and Examine the Data

```{r}
library(dplyr)

red_wine = read.csv("winequality-red.csv", sep = ";")
white_wine = read.csv("winequality-white.csv", sep = ";")
# Add categorical variables for wine type
red_wine$type = "Red"
white_wine$type = "White"
wine_data = bind_rows(red_wine, white_wine) # Combine the two datasets
wine_data$type = as.factor(wine_data$type)
str(wine_data)
```

## Remove Outliers

A few data points departed noticeably from the vast majority of the rest of the data. In the real world, we would carefully exam these points and consider why they might be abberations. However, that type of analysis is outside the scope of this modeling exercise, so we deviated from normal practices and simply removed the outliers to allow us to pursue a model that handles the remainder of the data.

TODO: (Add any additional commentary from Alexander.)

```{r}
res_sugar_6580 = which(wine_data$residual.sugar == 65.80)
free_sulf_dio_289 = which(wine_data$free.sulfur.dioxide == 289.0)
dens_10103 = which(wine_data$density == 1.0103)
remove_idx = c(res_sugar_6580, free_sulf_dio_289, dens_10103)
wine_data = wine_data[-remove_idx, ]
nrow(wine_data)
```

## Fit a Full Additive Model

We began by creating an additive model using all predictors (without transformations). This model served as a baseline to judge improvements for upcoming iterations.

```{r}
full_add_model = lm(quality ~ ., data = wine_data)
summary(full_add_model)
```
## Log Transformed Predictors

The predictor histograms showed that some of the predictors appear to be normally distributed, but other predictors are skewed. We experimented with a full model using log transformed variables for the ones that appeared especially skewed. We also tried a model with log transformed variables and interactive terms.

```{r}
model_add = lm(quality ~ ., data = wine_data)
model_log = lm(quality ~ fixed.acidity + log(volatile.acidity) + citric.acid + 
                 residual.sugar + chlorides + log(free.sulfur.dioxide) + 
                 total.sulfur.dioxide + density + (pH) + (sulphates) + 
                 (alcohol) + type, data = wine_data)
model_log_int = lm(quality ~ (fixed.acidity + log(volatile.acidity) + 
                                citric.acid + residual.sugar + chlorides + 
                                log(free.sulfur.dioxide) + 
                                total.sulfur.dioxide + density + (pH) + 
                                (sulphates) + (alcohol) + type)^2, 
                   data = wine_data)
```
## Fit a Full Interaction Model

Next, we created an interaction model using all predictors (without transformations).

```{r}
full_int_model = lm(quality ~ .^2, data = wine_data)
summary(full_int_model)
```

## Compare the Adjusted R-Squared for Various Models

```{r}
summary(model_add)$adj.r.squared
summary(model_log)$adj.r.squared
summary(full_int_model)$adj.r.squared
summary(model_log_int)$adj.r.squared
```

## Variance Inflation Factors

We calculated the Variance Inflation Factors (VIF) to help us identify issues of multicollinearity between predictors.

```{r}
if (!require(car)) install.packages("car")
library(car)
vif_values = vif(full_add_model)
vif_values
```

## Akaike Information Criterion (AIC) and Bayesian Information Criteria (BIC) Modelling

```{r}
model_bac_aic = step(model_log_int, trace = 0)
model_bac_bic = step(model_log_int, k = log(nrow(wine_data)), trace = 0)
model_both_aic = step(model_log_int, direction = "both", trace = 0)
summary(model_bac_aic)$adj.r.squared
summary(model_bac_bic)$adj.r.squared
summary(model_both_aic)$adj.r.squared
```

## Analysis of Variance (ANOVA)

```{r}
anova(model_bac_aic, model_log_int)
anova(model_bac_bic, model_log_int)
anova(model_both_aic, model_log_int)
```

# Results

## Predictor Scatterplots

```{r}
plot(wine_data$volatile.acidity, wine_data$quality,
     main = "Volatile Acidity vs Wine Quality",
     xlab = "Volatile Acidity",
     ylab = "Wine Quality",
     col = "blue", pch = 19, cex = 0.5)

plot(wine_data$residual.sugar, wine_data$quality,
     main = "Residual Sugar vs Wine Quality",
     xlab = "Residual Sugar",
     ylab = "Wine Quality",
     col = "green", pch = 19, cex = 0.5)

plot(wine_data$fixed.acidity, wine_data$quality,
     main = "Fixed Acidity vs Wine Quality",
     xlab = "Fixed Acidity",
     ylab = "Wine Quality",
     col = "purple", pch = 19, cex = 0.5)

plot(wine_data$citric.acid, wine_data$quality,
     main = "Citric Acid vs Wine Quality",
     xlab = "Citric Acid",
     ylab = "Wine Quality",
     col = "orange", pch = 19, cex = 0.5)

plot(wine_data$chlorides, wine_data$quality,
     main = "Chlorides vs Wine Quality",
     xlab = "Chlorides",
     ylab = "Wine Quality",
     col = "red", pch = 19, cex = 0.5)

plot(wine_data$free.sulfur.dioxide, wine_data$quality,
     main = "Free Sulfur Dioxide vs Wine Quality",
     xlab = "Free Sulfur Dioxide",
     ylab = "Wine Quality",
     col = "cyan", pch = 19, cex = 0.5)

plot(wine_data$total.sulfur.dioxide, wine_data$quality,
     main = "Total Sulfur Dioxide vs Wine Quality",
     xlab = "Total Sulfur Dioxide",
     ylab = "Wine Quality",
     col = "brown", pch = 19, cex = 0.5)

plot(wine_data$density, wine_data$quality,
     main = "Density vs Wine Quality",
     xlab = "Density",
     ylab = "Wine Quality",
     col = "pink", pch = 19, cex = 0.5)

plot(wine_data$pH, wine_data$quality,
     main = "pH vs Wine Quality",
     xlab = "pH",
     ylab = "Wine Quality",
     col = "darkgreen", pch = 19, cex = 0.5)

plot(wine_data$sulphates, wine_data$quality,
     main = "Sulphates vs Wine Quality",
     xlab = "Sulphates",
     ylab = "Wine Quality",
     col = "darkblue", pch = 19, cex = 0.5)

```

## Predictor Histograms

```{r}
numeric_columns = wine_data[sapply(wine_data, is.numeric)]
par(mfrow = c(2, 2))
sapply(names(numeric_columns), function(column) {
  hist(numeric_columns[[column]], main = paste("Histogram of", column),
       xlab = column, col = "lightblue", border = "white")
})
```

## Predictor Correlation Heatmap
```{r}
if (!require(corrplot)) install.packages("corrplot")
library(corrplot)

numeric_data = wine_data %>%
  select(-type, -quality) # Considers numeric predictors only
cor_matrix = cor(numeric_data, use = "complete.obs")
par(mar = c(0, 0, 5, 0))
corrplot(cor_matrix, 
         method = "color", 
         addCoef.col = "black", 
         tl.cex = 0.9,
         tl.col = "black",
         tl.srt = 45,
         number.cex = 0.8,
         main = "Correlation Heatmap", 
         cl.cex = 0.8,
         cl.ratio = 0.2,
         mar = c(0, 0, 1, 0),
         col = colorRampPalette(c("navy", "white", "brown"))(200)
)
```

## Linear Model Assumption Diagnostic Plots
```{r}
plot(model_both_aic, which = 1, main = "Residuals vs Fitted")
```

```{r}
plot(model_both_aic, which = 2, main = "Normal Q-Q Plot")
```

# Discussion

After considering several different models, our best model was...

TODO: (Describe the top model.)

This model achieved...

TODO: (Describe the performance of this model, using relevant values pulled from the model summaries.)

The steps and techniques we followed to arrive at this model included building a preliminary full additive model, transforming skewed predictors, experimenting with polynomial terms and interactions, and eliminating non-significant predictors. With each new candidate model, we analyzed how it compared to its predecessor models to assess whether we were headed in a productive direction.

Importantly, we did a final check on our final model to ensure that it obeyed the linear model assumptions. 